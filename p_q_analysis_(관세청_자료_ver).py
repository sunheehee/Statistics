# -*- coding: utf-8 -*-
"""p-q analysis (관세청 자료 VER)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E0shCTqQMojUYOGlCzUo5fF9i9QuPII6

# 프로젝트 개요

**`사용 데이터`**
- freighdata2025:부산 출항 운임 데이터(월 단위)
- tradedata2025:부산 출항 물동량 데이터(월 단위)
- companydata2025: 기업 데이터

**`분석 목표`**

- 두 데이터셋을 결합하여 거래 데이터를 유형별(고물동량·고운임 등)로 분류

- Pareto 법칙(80:20 Rule)을 적용해 전체 매출의 80%를 차지하는 핵심 20% 제품군을 도출

- 이를 통해 surff가 단순한 데이터 제공을 넘어,
실제 매출 기여도가 높은 제품군을 식별하고 영업 전략에 활용하고 있음을 입증

**`기대 효과`**

이를 입증함으로써 surff는 data-driven된 영업전략을 파트너들에게 제공하고 있는 신뢰할 수 있는 플랫폼 기업임을 보여주는 것이 최종 목표!
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install koreanize-matplotlib
# import koreanize_matplotlib

"""# 데이터 로드 및 확인"""

df_freight = pd.read_excel("/content/drive/MyDrive/SURFF_DA/freightdata2025.xlsx")
df_trade = pd.read_excel("/content/drive/MyDrive/SURFF_DA/export2025(raw).xlsx")
df_company=pd.read_csv("/content/drive/MyDrive/SURFF_DA/tradedata2025(raw).csv")

"""## **frieght:운임데이터**"""

df_freight.head()

df_freight = df_freight[df_freight['cntrType'] == 40]
df_freight.head()

df_freight.info()

df_freight.duplicated().sum()

print(df_freight.describe())

print("-"*20)

print(df_freight.describe(include='object'))

df_freight['podCode'].nunique()

freight_route = (df_freight
                 .groupby('podCode', as_index=False)
                 .agg(ocf=('ocf','mean')))
freight_route

"""

---
### **trade:물동량 데이터**
"""

df_trade = pd.read_excel("/content/drive/MyDrive/SURFF_DA/export2025(raw).xlsx")
df_trade.head()

df_trade_check = pd.read_excel("/content/drive/MyDrive/SURFF_DA/export2025.xlsx")
df_trade_check.head()

df_trade.head()

df_trade[df_trade.duplicated(keep=False)]

df_trade = df_trade.drop_duplicates()

print(df_trade.describe())
print("_" * 8)
print(df_trade.describe(include='object'))

df_trade.head()

trade_route = (
    df_trade
    .groupby('podCode', as_index=False)
    .agg(
        volume=('cntrCount','sum'),
        blCount=('blCount','sum')
    )
)

trade_route['avg_size'] = trade_route['volume'] / trade_route['blCount']
trade_route.head()

df_trade_check.rename(columns={'volume/blCount':'avg_size'},inplace=True)
df_trade_check.head()

df_trade['podCode'].nunique()

df_trade_check['podCode'].nunique()

# 두 데이터프레임 병합
cmp = trade_route.merge(
    df_trade_check, on='podCode', how='inner', suffixes=('_calc', '_excel')
)

# 소수점 3자리 반올림 후 비교
cmp['avg_size_calc_r']  = cmp['avg_size_calc'].round(3)
cmp['avg_size_excel_r'] = cmp['avg_size_excel'].round(3)

# 비교 결과
cmp['match_volume'] = cmp['volume_calc'] == cmp['volume_excel']
cmp['match_bl']     = cmp['blCount_calc'] == cmp['blCount_excel']
cmp['match_avg']    = cmp['avg_size_calc_r'] == cmp['avg_size_excel_r']

# 불일치 행만 보기
mismatch = cmp[~(cmp['match_volume'] & cmp['match_bl'] & cmp['match_avg'])]
print(mismatch)

# 전체 일치율
print("전체 일치율:", (cmp['match_volume'] & cmp['match_bl'] & cmp['match_avg']).mean())

trade_route[trade_route['podCode']=='USHOU']

"""

---
##**company: 기업 데이터**"""

df_company.head()

df_company.rename(columns={'shipgWeek':'yearWeek'},inplace=True)
df_company = df_company[df_company['polCode'] == 'KRPUS']
df_company.head()

df_company.duplicated().sum()

print(df_company.describe())

print("-"*20)

print(df_company.describe(include='object'))

df_company.columns

df_company.head()

"""### 데이터 정제

###bl 중복 확인
"""

# blNo 중복 개수
print("blNo 중복개수:", df_company['blNo'].duplicated().sum())

# blNo 별 몇 행이 있는지
print(df_company['blNo'].value_counts().head(20))

dup_bl = df_company[df_company['blNo'] =='MAEUMEMO00001']
dup_bl[['blNo','shipgkey','polCode','podCode','shipgDate','yearWeek','linerNm',]]

dup_bl.describe(include='object')

# 드롭하려는 BL 번호 지정
target_bl = "MAEUMEMO00001"

# 해당 BL을 가진 행 제거
df_company = df_company[df_company['blNo'] != target_bl]

# blNo 중복 개수
print("blNo 중복개수:", df_company['blNo'].duplicated().sum())

# blNo 별 몇 행이 있는지
print(df_company['blNo'].value_counts().head(20))

"""B/L NO. MAEUMEMO00001를 제외하고는 모두 LCL화물임을 확인함.

MAEUMEMO00001은 hscodeNm이 모두 컨테이너인것을 보아 공컨테이너 재배치를 위한 운항임을 유추가능

제외
"""

df_company.isnull().sum()

"""###중량 이상치 확인"""

import matplotlib.pyplot as plt

cols = ['msrmnTon','wtTon']

for c in cols:
    data = df_company[c].dropna()

    plt.figure(figsize=(12,4))

    # 1) 히스토그램
    plt.subplot(1,2,1)
    plt.hist(data, bins=100, color='steelblue', alpha=0.7)
    plt.title(f"{c} Histogram (raw)")
    plt.xlabel("value")
    plt.ylabel("count")

    # 2) 박스플롯
    plt.subplot(1,2,2)
    plt.boxplot(data, vert=False, showfliers=True)
    plt.title(f"{c} Boxplot (raw)")
    plt.xlabel("value")

    plt.tight_layout()
    plt.show()

df_company[df_company['msrmnTon']>6000]

"""###핵심 변수 설정"""

df_company=df_company[["blNo","shipgkey","polCode","podCode","yearWeek","hsCode","compName","contnCo"]]
df_company.head()

# compName이 'Unknown'인 행의 개수
unknown_count = (df_company['compName'] == 'Unknown').sum()

# 전체 행의 개수
total_count = len(df_company)

# 비율 계산
unknown_ratio = unknown_count / total_count

print(f"Unknown 개수: {unknown_count}")
print(f"Unknown 비율: {unknown_ratio:.2%}")

"""### hscode로 unknown 기업 수 근사값 도출하기"""

# 1. Unknown 기업명 생성 (HSCode+노선)
df_company_ukhs = df_company.copy()
mask = df_company_ukhs['compName'] == 'Unknown'
df_company_ukhs.loc[mask, 'compName'] = (
    'Unknown_'
    + df_company_ukhs.loc[mask, 'podCode'] + '_'
    + df_company_ukhs.loc[mask, 'hsCode'].astype(str)
)

df_company_ukhs.head()

"""### 처리 성능"""

# 2. company 데이터 집계 (노선×주차 단위 컨테이너 수)
company_agg = (
    df_company_ukhs
    .groupby(['podCode'], as_index=False)
    .agg(volume_company=('contnCo','sum'))
)

# 4. 두 데이터 비교
df_check = pd.merge(trade_route, company_agg,
                    on=['podCode'],
                    how='inner')

df_check['diff'] = df_check['volume'] - df_check['volume_company']

# 일치율 계산 (차이=0인 비율)
match_rate = (df_check['diff'] == 0).mean()
print(f"완전 일치율: {match_rate:.2%}")

# 허용 오차 (±1TEU) 일치율
tol = 1
match_rate_tol = (df_check['diff'].abs() <= tol).mean()
print(f"허용 오차 ±{tol}TEU 기준 일치율: {match_rate_tol:.2%}")

# 차이가 큰 사례 확인
df_check.sort_values('diff', ascending=False).head(30)

import numpy as np

KEY_ROUTE = ['podCode']   # polCode 제외

def company_stats(df, keys):
    return (
        df
        .groupby(keys, as_index=False)
        .agg(
            company_cnt=('compName','nunique'),
            unknown_cnt=('compName', lambda s: (s == 'Unknown').sum()),
            rows=('compName','size')
        )
        .assign(unknown_ratio=lambda d: d['unknown_cnt'] / d['rows'])
    )

# 전(before): 원본
stats_before = company_stats(df_company, KEY_ROUTE)

# 후(after): Unknown_*로 세분화한 가공본
stats_after = company_stats(df_company_ukhs, KEY_ROUTE)

# 비교 테이블 생성
cmp_route = (
    stats_before
    .merge(stats_after, on=KEY_ROUTE, suffixes=('_before','_after'), how='outer')
    .fillna(0)
    .assign(
        delta_company=lambda d: d['company_cnt_after'] - d['company_cnt_before']
    )
)

# 원하는 컬럼 순서만 남기기
cmp_route = cmp_route[
    [
        'podCode',
        'company_cnt_before',
        'company_cnt_after',
        'unknown_cnt_before',
        'unknown_cnt_after',
        'unknown_ratio_before',
        'delta_company'
    ]
].copy()

cmp_route.head()

cmp_route[cmp_route['unknown_cnt_after'] != 0]

# 노선 기준: 기업 수 증가 Top 5
top_inc_route = cmp_route.sort_values('delta_company', ascending=False).head(5)
top_inc_route

"""## 데이터 병합(df_pq)

(trade & freight, polCode+podCode+yearWeek 기준)

---


"""

freight_route

trade_route

df_trade['podCode'].nunique()

df_pq = (
    trade_route.merge(freight_route, on='podCode', how='inner')
)
df_pq['pq'] = df_pq['volume'] * df_pq['ocf']
df_pq['ocf'] = df_pq['ocf'].round(0).astype(int)
df_pq['pq']  = df_pq['pq'].round(0).astype(int)

df_pq

df_pq.info()

df_pq.duplicated().sum()

"""# Pareto분석
- "상위 20%의 노선이 전체 매출의 80%를 차지한다"는 가설 검증
- 매출 상위 20% 핵심 노선 도출

"""

df_route = (df_pq
            .groupby('podCode', as_index=False)
            .agg(pq=('pq','sum'))             # 노선 총 매출(P×Q)
            .sort_values('pq', ascending=False)
            .reset_index(drop=True))

df_route['share'] = df_route['pq'] / df_route['pq'].sum()
df_route['cum_share'] = df_route['share'].cumsum()

# 80% 경계 인덱스 (핵심군)
cutoff_index = df_route[df_route['cum_share'] <= 0.8].index.max()

# 상위 20% 관련 지표
top_20pct_count = max(1, int(len(df_route) * 0.20))
top_20pct_cum_share = df_route.loc[:top_20pct_count-1, 'share'].sum()

# 색상: 80% 누적 이내(핵심군)=tomato, 그 외=lightblue
colors = ['tomato' if i <= cutoff_index else 'lightblue' for i in df_route.index]

# -----------------------------
# 4) 그래프
# -----------------------------
fig, ax1 = plt.subplots(figsize=(12,7))

# 막대: 개별 노선 매출 비중
ax1.bar(df_route.index, df_route['share'], color=colors, alpha=0.7)
ax1.set_ylabel("매출 비중(Share)")
ax1.set_xlabel("노선 순위 (매출 내림차순)")
ax1.set_ylim(0, df_route['share'].max()*1.1)

# 누적선: 누적 매출 기여도
ax2 = ax1.twinx()
ax2.plot(df_route.index, df_route['cum_share'], color="red", marker="o", markersize=4)
ax2.set_ylabel("누적 매출 기여도")
ax2.set_ylim(0, 1.02)

# 80% 기준선
ax2.axhline(0.8, color="gray", linestyle="--", linewidth=1)

plt.title("Pareto 분석 (노선별 매출 비중 및 누적 기여도)", fontsize=14)

# 하단 요약 텍스트
textstr = '\n'.join((
    f"- 상위 20% 노선 개수: {top_20pct_count}",
    f"- 상위 20% PQ 비중: {top_20pct_cum_share:.2%}"
))
plt.gcf().text(0.12, -0.08, textstr, fontsize=12, ha="left")

plt.tight_layout()
plt.show()

# 상위 20% 핵심군 표 출력
core_routes = df_route.head(top_20pct_count)
display(core_routes)

"""# P-Q MATRIX
- 모든 노선을 P-Q 평면에 배치해서 포지션 분포 시각화
- 운임X물동량으로 사분면 분류
>고P·고Q / 저P·고Q / 고P·저Q / 저P·저Q 상대적 위치 확인 목적
    
    ➡️ 4분할 구조 확인

### 분할 기준선 설정
- 물동량, 운임 왜도 확인
    
    ➡️ 평균값, 중앙값 중 선택
"""

import matplotlib.pyplot as plt

# 왜도 계산
vol_skew = df_pq['volume'].skew()
ocf_skew = df_pq['ocf'].skew()

print("물동량(volume) 왜도:", vol_skew)
print("운임(ocf) 왜도:", ocf_skew)

# 히스토그램 + 커널밀도(KDE) 그래프
fig, axes = plt.subplots(1, 2, figsize=(12,5))

# 물동량 분포
axes[0].hist(df_pq['volume'], bins=30, color="skyblue", alpha=0.7, edgecolor="black")
axes[0].set_title(f"물동량 분포 (왜도={vol_skew:.2f})")
axes[0].set_xlabel("volume")
axes[0].set_ylabel("frequency")

# 운임 분포
axes[1].hist(df_pq['ocf'], bins=30, color="salmon", alpha=0.7, edgecolor="black")
axes[1].set_title(f"운임 분포 (왜도={ocf_skew:.2f})")
axes[1].set_xlabel("ocf")
axes[1].set_ylabel("frequency")

plt.tight_layout()
plt.show()

"""✅ p-q 기준선 중앙값 설정

### P-Q Matrix
"""

from matplotlib.lines import Line2D
# =========================================================
# 0) route 만들기 (중복 집계 제거 + polCode 제거)
#    - df_pq는 최소한 ['podCode','volume','pq']를 포함해야 함
#    - 이미 노선 단위 집계본이면 drop_duplicates만 수행
#    - 아니라면 podCode 기준으로 재집계
# =========================================================
if {'podCode','volume','pq'}.issubset(df_pq.columns):
    route = df_pq[['podCode','volume','pq']].drop_duplicates().copy()
else:
    route = (
        df_pq.groupby('podCode', as_index=False)
             .agg(volume=('volume','sum'),
                  pq=('pq','sum'))
    )

# ocf = pq / volume (안전 처리)
route['ocf'] = np.where(route['volume'] > 0, route['pq'] / route['volume'], np.nan)
route['ocf'].replace([np.inf, -np.inf], np.nan, inplace=True)

# =========================================================
# 1) Pareto 핵심군 (개수 기준 Top20%)
#    - 기존 로직 유지: 상위 20% "개수"로 핵심군 플래그
# =========================================================
r = route.sort_values('pq', ascending=False).reset_index(drop=True)
top_n = max(1, int(np.ceil(len(r) * 0.20)))
core_keys = r.loc[:top_n-1, ['podCode']]

route = route.merge(core_keys.assign(is_core=True),
                    on='podCode', how='left')
route['is_core'] = route['is_core'].fillna(False)

# =========================================================
# 2) 사분면 기준 계산 (중앙값)
# =========================================================
x_med = route['volume'].median()
y_med = route['ocf'].median()

# quadrant_pq: 1~4 (저물량/고물량 × 저운임/고운임)
#  volume<=med, ocf<=med  -> 1
#  volume<=med, ocf>med   -> 2
#  volume>med,  ocf<=med  -> 3
#  volume>med,  ocf>med   -> 4
route['quadrant_pq'] = (
    (route['volume'] > x_med).astype(int) * 2 +
    (route['ocf'] > y_med).astype(int) + 1
)

# =========================================================
# 3) P–Q 매트릭스 시각화
#    - 색상/스타일: 기존 취향 유지 (핵심=tomato, 비핵심=lightgray)
# =========================================================
fig, ax = plt.subplots(figsize=(11, 7))

# 산점도
ax.scatter(
    x=route['volume'], y=route['ocf'],
    c=route['is_core'].map({True: 'tomato', False: 'lightgray'}),
    s=45, alpha=0.8, edgecolor='k', linewidth=0.4
)

# 중앙값 기준선
ax.axvline(x=x_med, color='gray', ls='--', lw=1)
ax.axhline(y=y_med, color='gray', ls='--', lw=1)

# 핵심군 라벨 (podCode만 표시)
for _, row in route[route['is_core']].iterrows():
    ax.annotate(
        row['podCode'], (row['volume'], row['ocf']),
        xytext=(0, 7), textcoords='offset points',
        ha='center', fontsize=8, color='firebrick',
        bbox=dict(boxstyle='round,pad=0.2', fc='white', ec='none', alpha=0.7)
    )

# 축/제목/라벨
ax.set_xscale('log')
ax.set_title('P–Q Matrix (podCode 단위, 핵심 Top20% 강조)')
ax.set_xlabel('물동량 (Volume)')
ax.set_ylabel('운임 (OCF)')

# 범례
legend_elems = [
    Line2D([0],[0], marker='o', color='w',
           label=f'핵심 Top20% (n={int(route["is_core"].sum())})',
           markerfacecolor='tomato', markeredgecolor='k', markersize=8),
    Line2D([0],[0], marker='o', color='w',
           label=f'비핵심 (n={int((~route["is_core"]).sum())})',
           markerfacecolor='lightgray', markeredgecolor='k', markersize=8),
    Line2D([0],[0], color='gray', lw=1, ls='--', label='Median Volume/OCF')
]
ax.legend(handles=legend_elems, loc='best')
ax.grid(alpha=0.25)

plt.tight_layout()
plt.show()

# =========================================================
# 4) 요약 출력
# =========================================================
print('요약')
print(f"- 전체 노선 수: {len(route)}")
print(f"- Pareto 핵심군 노선 수(개수 상위 20%): {int(route['is_core'].sum())} / {len(route)} (≈ {top_n})")
print(f"- Volume 중앙값: {x_med:.3f}, OCF 중앙값: {y_med:.3f}")
print("- 사분면 분포:")
print(route['quadrant_pq'].value_counts().sort_index().rename(index={
    1:'Q1 (저물량·저운임)',
    2:'Q2 (저물량·고운임)',
    3:'Q3 (고물량·저운임)',
    4:'Q4 (고물량·고운임)'
}))

from matplotlib.lines import Line2D

# =========================================================
# 0) route 만들기 (중복 집계 제거 + polCode 제거)
# =========================================================
if {'podCode','volume','pq'}.issubset(df_pq.columns):
    route = df_pq[['podCode','volume','pq']].drop_duplicates().copy()
else:
    route = (
        df_pq.groupby('podCode', as_index=False)
             .agg(volume=('volume','sum'),
                  pq=('pq','sum'))
    )

# ocf = pq / volume (안전 처리)
route['ocf'] = np.where(route['volume'] > 0, route['pq'] / route['volume'], np.nan)
route['ocf'].replace([np.inf, -np.inf], np.nan, inplace=True)

# =========================================================
# 1) Pareto 핵심군 (개수 기준 Top20%)
# =========================================================
r = route.sort_values('pq', ascending=False).reset_index(drop=True)
top_n = max(1, int(np.ceil(len(r) * 0.20)))
core_keys = r.loc[:top_n-1, ['podCode']]

route = route.merge(core_keys.assign(is_core=True),
                    on='podCode', how='left')
route['is_core'] = route['is_core'].fillna(False)

# =========================================================
# 2) 사분면 기준 계산 (중앙값)
# =========================================================
x_med = route['volume'].median()
y_med = route['ocf'].median()

route['quadrant_pq'] = (
    (route['volume'] > x_med).astype(int) * 2 +
    (route['ocf'] > y_med).astype(int) + 1
)

# =========================================================
# 3) P–Q 매트릭스 시각화 (로그 제거, 선형축)
# =========================================================
fig, ax = plt.subplots(figsize=(11, 7))

# 산점도
ax.scatter(
    x=route['volume'], y=route['ocf'],
    c=route['is_core'].map({True: 'tomato', False: 'lightgray'}),
    s=45, alpha=0.8, edgecolor='k', linewidth=0.4
)

# 중앙값 기준선
ax.axvline(x=x_med, color='gray', ls='--', lw=1)
ax.axhline(y=y_med, color='gray', ls='--', lw=1)

# 핵심군 라벨
for _, row in route[route['is_core']].iterrows():
    ax.annotate(
        row['podCode'], (row['volume'], row['ocf']),
        xytext=(0, 7), textcoords='offset points',
        ha='center', fontsize=8, color='firebrick',
        bbox=dict(boxstyle='round,pad=0.2', fc='white', ec='none', alpha=0.7)
    )

# 축/제목/라벨 (선형축)
ax.set_title('P–Q Matrix (podCode 단위, 핵심 Top20% 강조)')
ax.set_xlabel('물동량 (Volume)')
ax.set_ylabel('운임 (OCF)')

# 범례
legend_elems = [
    Line2D([0],[0], marker='o', color='w',
           label=f'핵심 Top20% (n={int(route["is_core"].sum())})',
           markerfacecolor='tomato', markeredgecolor='k', markersize=8),
    Line2D([0],[0], marker='o', color='w',
           label=f'비핵심 (n={int((~route["is_core"]).sum())})',
           markerfacecolor='lightgray', markeredgecolor='k', markersize=8),
    Line2D([0],[0], color='gray', lw=1, ls='--', label='Median Volume/OCF')
]
ax.legend(handles=legend_elems, loc='best')
ax.grid(alpha=0.25)

plt.tight_layout()
plt.show()

# =========================================================
# 4) 요약 출력
# =========================================================
print('요약')
print(f"- 전체 노선 수: {len(route)}")
print(f"- Pareto 핵심군 노선 수(개수 상위 20%): {int(route['is_core'].sum())} / {len(route)} (≈ {top_n})")
print(f"- Volume 중앙값: {x_med:.3f}, OCF 중앙값: {y_med:.3f}")
print("- 사분면 분포:")
print(route['quadrant_pq'].value_counts().sort_index().rename(index={
    1:'Q1 (저물량·저운임)',
    2:'Q2 (저물량·고운임)',
    3:'Q3 (고물량·저운임)',
    4:'Q4 (고물량·고운임)'
}))



"""# K-means

## 데이터 병합(kmeans_input)
- df_pq와 df_hhi 병합
"""

df_pq.head()

df_company_ukhs

KEYS = ['podCode']

dfc = df_company_ukhs.copy()

# ---------------------------------------------------------
# 1) 회사 레벨까지 누적 (podCode, compName 별 총 contnCo)
# ---------------------------------------------------------
comp_hhi = (
    dfc.groupby(KEYS + ['compName'], as_index=False)
       .agg(contnCo=('contnCo','sum'))
)

# ---------------------------------------------------------
# 2) 노선 총량과 점유율
# ---------------------------------------------------------
totals = (
    comp_hhi.groupby(KEYS, as_index=False)['contnCo']
            .sum()
            .rename(columns={'contnCo':'contnCo_total'})
)

shares = comp_hhi.merge(totals, on=KEYS, how='left')
# 0 나눗셈 방지
shares['share'] = shares['contnCo'] / shares['contnCo_total'].replace(0, np.nan)

# ---------------------------------------------------------
# 3) HHI / n_comp
#    - HHI는 0~1 스케일. 필요시 *10000 해서 0~10000로 사용
# ---------------------------------------------------------
hhi_part  = (
    shares.assign(share2=lambda d: d['share'].fillna(0)**2)
          .groupby(KEYS, as_index=False)
          .agg(hhi=('share2','sum'),
               n_comp=('compName','nunique'))
)

# ---------------------------------------------------------
# 4) CR3 / Top1 (share 기준 내림차순)
#    정렬 길이와 방향 버그 제거
# ---------------------------------------------------------
ordered = shares.sort_values(KEYS + ['share'],
                             ascending=[True]*len(KEYS) + [False])

cr3 = (ordered.groupby(KEYS).head(3)
               .groupby(KEYS, as_index=False)
               .agg(cr3=('share','sum')))

top1 = (ordered.groupby(KEYS).head(1)
                 .rename(columns={'share':'top1_share'})
                 [KEYS + ['top1_share']])

# ---------------------------------------------------------
# 5) comp_list (share 내림차순 순서 보존)
# ---------------------------------------------------------
comp_list_sorted = (
    ordered.groupby(KEYS, as_index=False)
           .agg(comp_list=('compName', list))
)

# ---------------------------------------------------------
# 6) 최종 병합
# ---------------------------------------------------------
df_hhi = (hhi_part
          .merge(cr3, on=KEYS, how='left', validate='one_to_one')
          .merge(top1, on=KEYS, how='left', validate='one_to_one')
          .merge(totals, on=KEYS, how='left', validate='one_to_one')
          .merge(comp_list_sorted, on=KEYS, how='left', validate='one_to_one'))

# (선택) HHI 스케일 변경
# df_hhi['hhi'] = (df_hhi['hhi'] * 10_000).round(0).astype(int)
df_hhi

df_hhi.duplicated(subset='podCode').sum()

df_hhi['podCode'].nunique()

df_company_ukhs['podCode'].nunique()

KEYS = ['podCode']

# 1) trade (volume)
kmeans_trade = (
    trade_route.groupby(KEYS, as_index=False)
        .agg(
            volume=('volume', 'sum'),
            blCount=('blCount', 'sum')
        )
)

# 2) freight (ocf)
kmeans_freight = (freight_route.groupby(KEYS, as_index=False)
                  .agg(ocf=('ocf','mean')))

# 3) comp structure (HHI, CR3, n_comp 등)
kmeans_comp = df_hhi.copy()

# 4) 최종 병합
kmeans_input = (kmeans_trade
                .merge(kmeans_freight, on=KEYS, how='inner', validate='one_to_one')
                .merge(kmeans_comp, on=KEYS, how='inner', validate='one_to_one'))

# 5) 파생변수: 거래단위 평균규모 avg_size = volume / blCount (0/결측 안전 처리)
kmeans_input['avg_size'] = np.where(
    (kmeans_input['blCount'] > 0) & (~kmeans_input['blCount'].isna()),
    kmeans_input['volume'] / kmeans_input['blCount'],
    np.nan
)

# 6) 컬럼 순서 정리
preferred_cols = ['podCode', 'ocf', 'volume', 'blCount', 'avg_size', 'hhi']
others = [c for c in kmeans_input.columns if c not in preferred_cols]
kmeans_input = kmeans_input[preferred_cols + others]



kmeans_input

"""## 다중공선성 확인"""

import seaborn as sns
import matplotlib.pyplot as plt

corr = kmeans_input[['volume','ocf','avg_size','hhi']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.show()

from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd

X = kmeans_input[['volume','ocf','avg_size','hhi']].dropna()
vif = pd.DataFrame()
vif["feature"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif)

"""## 가중치 적용(큰 걸 더 크게 버전!)

지표별 표준편차 확인
"""

import matplotlib.pyplot as plt

# 사용할 피처
features = ['ocf','volume','hhi','avg_size']

# 지표별 평균, 표준편차 계산
stats = kmeans_input[features].agg(['mean','std']).T
stats['var'] = stats['std']**2

#지표별 히스토그램
plt.figure(figsize=(12,8))

for i, col in enumerate(features, 1):
    plt.subplot(2,2,i)
    plt.hist(kmeans_input[col], bins=30, color='steelblue', alpha=0.7)
    plt.axvline(kmeans_input[col].mean(), color='red', linestyle='dashed', linewidth=1)
    plt.title(f"{col} (mean={kmeans_input[col].mean():.2f}, std={kmeans_input[col].std():.2f})")

plt.tight_layout()
plt.show()

print(stats)

stds = kmeans_input[features].std(ddof=0)
weights = stds / stds.sum()
print("가중치(원자료 기준):\n", weights)

"""### 스케일링(Min-Max)"""

from sklearn.preprocessing import MinMaxScaler

# 2) 스케일링 (MinMax 예시)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(kmeans_input[features])

# 3) 가중치 적용
X_weighted = X_scaled * weights.values
X_weighted = pd.DataFrame(X_weighted, columns=features, index=kmeans_input.index)

print(X_weighted.head())

"""### 최적의 K찾기"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# X_weighted: (행=노선, 열=ocf/volume/hhi/avg_size)  ← 이미 만들어둔 것 사용

# -----------------------------
# 1) k 탐색 (Elbow + Silhouette)
# -----------------------------
def find_best_k(X, k_min=2, k_max=8, name="Weighted"):
    Ks, inertias, sils = [], [], []
    for k in range(k_min, k_max+1):
        km = KMeans(n_clusters=k, n_init=20, random_state=42)
        labels = km.fit_predict(X)
        Ks.append(k)
        inertias.append(km.inertia_)
        sils.append(silhouette_score(X, labels))

    plt.figure(figsize=(12,5))
    # Elbow
    plt.subplot(1,2,1)
    plt.plot(Ks, inertias, marker='o')
    plt.title(f"{name} | Elbow (Inertia)")
    plt.xlabel("k"); plt.ylabel("Inertia"); plt.grid(True)

    # Silhouette
    plt.subplot(1,2,2)
    plt.plot(Ks, sils, marker='o')
    plt.title(f"{name} | Silhouette")
    plt.xlabel("k"); plt.ylabel("Silhouette score"); plt.grid(True)
    plt.show()

    # 표로도 반환
    return pd.DataFrame({"k": Ks, "inertia": inertias, "silhouette": sils})

results_weighted = find_best_k(X_weighted.values, k_min=2, k_max=8, name="Company weights")
results_weighted

"""# 최적의 k는 4!!!"""

# -----------------------------
# 2) 최종 KMeans & 요약 테이블
# -----------------------------
def run_kmeans_and_summary(X, k, index, feature_names):
    km = KMeans(n_clusters=k, n_init=20, random_state=42)
    labels = km.fit_predict(X)
    sil = silhouette_score(X, labels)

    # 라벨 Series
    labels_s = pd.Series(labels, index=index, name=f"cluster_k{k}")

    # 원자료(가중 전)에서 군집별 요약(평균/중앙값/개수)
    df_orig = kmeans_input[feature_names].copy()
    df_orig["cluster"] = labels

    mean_tbl = df_orig.groupby("cluster")[feature_names].mean().add_prefix("mean_")
    med_tbl  = df_orig.groupby("cluster")[feature_names].median().add_prefix("median_")
    cnt_tbl  = df_orig.groupby("cluster").size().to_frame("count")

    summary = pd.concat([cnt_tbl, mean_tbl, med_tbl], axis=1).sort_values("count", ascending=False)

    print(f"[k={k}] silhouette = {sil:.4f}")
    return labels_s, km, summary

feature_names = ["ocf","volume","hhi","avg_size"]
best_k = 4
labels_w, km_w, summary_w = run_kmeans_and_summary(X_weighted.values, best_k, X_weighted.index, feature_names)
summary_w

summary_w.loc[:, ["count","mean_hhi","median_hhi","mean_ocf","mean_volume","mean_avg_size"]]

# -----------------------------
# 3) P-Q 산점도 (ocf vs volume)
# -----------------------------
plt.figure(figsize=(7,6))
for c in sorted(labels_w.unique()):
    mask = (labels_w == c).values
    plt.scatter(kmeans_input.loc[mask, "ocf"],
                kmeans_input.loc[mask, "volume"],
                s=18, alpha=0.7, label=f"C{c}")
plt.xlabel("OCF (price)")
plt.ylabel("Volume")
plt.title(f"P-Q scatter by cluster (k={best_k})")
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

# 0) podCode 준비 + 라벨을 route에 붙이기
km_df = kmeans_input.copy()
if 'podCode' not in km_df.columns:
    km_df['podCode'] = km_df.index.astype(str)

cluster_map = km_df[['podCode']].copy()
cluster_map['cluster'] = labels_w.values  # run_kmeans_and_summary에서 나온 라벨 Series

route_c = route.merge(cluster_map, on='podCode', how='left')

# 1) 중앙값(메디안) 계산
x_med = route_c['volume'].median()
y_med = route_c['ocf'].median()

# 2) P–Q 산점도: 클러스터 색상 + 메디안선
fig, ax = plt.subplots(figsize=(11, 7))

clusters = sorted(route_c['cluster'].dropna().unique())
color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get('color', [])
colors = {c: color_cycle[i % len(color_cycle)] for i, c in enumerate(clusters)}

for c in clusters:
    m = route_c['cluster'] == c
    ax.scatter(route_c.loc[m, 'volume'], route_c.loc[m, 'ocf'],
               s=36, c=colors[c], alpha=0.75,
               edgecolor='white', linewidth=0.6, label=f"C{int(c)}")

# 메디안 기준선
ax.axvline(x=x_med, color='gray', ls='--', lw=1)
ax.axhline(y=y_med, color='gray', ls='--', lw=1)

# 축/제목/범례
ax.set_xlabel('물동량 (Volume)')
ax.set_ylabel('운임 (OCF)')
ax.set_title('P–Q Matrix (podCode 단위) — 클러스터 색상 (선형축)')

handles, labels = ax.get_legend_handles_labels()
handles += [Line2D([0],[0], color='gray', lw=1, ls='--')]
labels  += ['Median Volume/OCF']
ax.legend(handles, labels, loc='best')

ax.grid(alpha=0.25)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

# ---------------------------------------------------------
# 0) 준비: podCode / cluster / pq(= ocf*volume) 계산
# ---------------------------------------------------------
km_df = kmeans_input.copy()
if 'podCode' not in km_df.columns:
    km_df['podCode'] = km_df.index.astype(str)

cluster_map = km_df[['podCode']].copy()
cluster_map['cluster'] = labels_w.values  # run_kmeans_and_summary에서 생성된 라벨 Series

route_c = route.merge(cluster_map, on='podCode', how='left')

# pq(매출 지표) 없으면 생성
if 'pq' not in route_c.columns:
    route_c['pq'] = route_c['ocf'] * route_c['volume']

# ---------------------------------------------------------
# 1) 클러스터별 Top5 (pq 기준) 선별
# ---------------------------------------------------------
# 클러스터가 NaN인 행 제거
rc_valid = route_c.dropna(subset=['cluster']).copy()

# pq가 NaN/inf면 0으로 처리(표시 안전)
rc_valid['pq'] = np.where(np.isfinite(rc_valid['pq']), rc_valid['pq'], 0.0)

# 각 클러스터에서 pq 상위 5개
top5_per_cluster = (
    rc_valid.sort_values(['cluster', 'pq'], ascending=[True, False])
            .groupby('cluster', as_index=False, group_keys=False)
            .head(5)
            .copy()
)

# ---------------------------------------------------------
# 2) 중앙값(메디안) 기준선
# ---------------------------------------------------------
x_med = rc_valid['volume'].median()
y_med = rc_valid['ocf'].median()

# ---------------------------------------------------------
# 3) P–Q 산점도: 클러스터 색상 + 메디안선 + Top5 라벨
# ---------------------------------------------------------
fig, ax = plt.subplots(figsize=(11, 7))

clusters = sorted(rc_valid['cluster'].unique())
color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get('color', [])
colors = {c: color_cycle[i % len(color_cycle)] for i, c in enumerate(clusters)}

# 전체 점 찍기
for c in clusters:
    m = rc_valid['cluster'] == c
    ax.scatter(rc_valid.loc[m, 'volume'], rc_valid.loc[m, 'ocf'],
               s=36, c=colors[c], alpha=0.75,
               edgecolor='white', linewidth=0.6, label=f"C{int(c)}")

# 메디안 기준선
ax.axvline(x=x_med, color='gray', ls='--', lw=1)
ax.axhline(y=y_med, color='gray', ls='--', lw=1)

# ---------------------------------------------------------
# 4) 클러스터별 Top5 주석(라벨) 표시
#    - 라벨: podCode (필요 시 pq도 함께)
#    - 중첩을 줄이기 위해 약간의 오프셋 적용
# ---------------------------------------------------------
# 라벨 오프셋 패턴 (위/오른쪽/왼쪽 번갈기)
offsets = [(8, 8), (8, -8), (-8, 8), (-8, -8), (12, 0)]

for c in clusters:
    sub = top5_per_cluster[top5_per_cluster['cluster'] == c]
    for i, (_, r) in enumerate(sub.iterrows()):
        x, y = r['volume'], r['ocf']
        dx, dy = offsets[i % len(offsets)]
        label = f"{r['podCode']}"  # 필요하면 f"{r['podCode']} (pq:{r['pq']:.1f})"
        ax.annotate(
            label,
            xy=(x, y),
            xytext=(x + dx, y + dy),
            textcoords='data',
            fontsize=9,
            color=colors[c],
            ha='left', va='bottom',
            arrowprops=dict(arrowstyle='-', lw=0.8, color=colors[c], alpha=0.8)
        )

# 축/제목/범례
ax.set_xlabel('물동량 (Volume)')
ax.set_ylabel('운임 (OCF)')
ax.set_title('P–Q Matrix — 클러스터 색상 및 클러스터별 Top5(PQ 기준)')

# 범례에 메디안 의미 추가
handles, labels = ax.get_legend_handles_labels()
handles += [Line2D([0],[0], color='gray', lw=1, ls='--')]
labels  += ['Median Volume/OCF']
ax.legend(handles, labels, loc='best')

ax.grid(alpha=0.25)
plt.tight_layout()
plt.show()

plt.figure(figsize=(7,4))
for c in sorted(labels_w.unique()):
    plt.hist(kmeans_input.loc[labels_w==c, "hhi"], bins=20, alpha=0.5, label=f"C{c}")
plt.title(f"HHI distribution by cluster (k={best_k})")
plt.xlabel("HHI"); plt.ylabel("count")
plt.legend(); plt.grid(True); plt.show()

"""## 클러스터 4개의 경우  비교"""

feature_sets = {
    "ocf_volume": ['ocf','volume'],
    "ocf_volume_hhi": ['ocf','volume','hhi'],
    "ocf_volume_avgSize": ['ocf','volume','avg_size'],
    "ocf_volume_hhi_avgSize": ['ocf','volume','hhi','avg_size']
}

from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

def weighted_X(df, features, weights):
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(df[features])
    return X_scaled * np.array([weights[f] for f in features])

def find_best_k(X, max_k=8, name=""):
    Ks, inertias, sils = [], [], []
    for k in range(2, max_k+1):
        km = KMeans(n_clusters=k, n_init=20, random_state=42)
        labels = km.fit_predict(X)
        Ks.append(k)
        inertias.append(km.inertia_)
        sils.append(silhouette_score(X, labels))

    fig, axes = plt.subplots(1,2, figsize=(12,5))

    # Elbow
    axes[0].plot(Ks, inertias, marker='o')
    axes[0].set_title(f"{name} | Elbow (Inertia)")
    axes[0].set_xlabel("k"); axes[0].set_ylabel("Inertia"); axes[0].grid(True)

    # Silhouette
    axes[1].plot(Ks, sils, marker='o', color='orange')
    axes[1].set_title(f"{name} | Silhouette")
    axes[1].set_xlabel("k"); axes[1].set_ylabel("Silhouette score"); axes[1].grid(True)

    plt.tight_layout()
    plt.show()

    return pd.DataFrame({"k": Ks, "inertia": inertias, "silhouette": sils})

feature_sets = {
    "ocf_volume": ['ocf','volume'],
    "ocf_volume_hhi": ['ocf','volume','hhi'],
    "ocf_volume_avgSize": ['ocf','volume','avg_size'],
    "ocf_volume_hhi_avgSize": ['ocf','volume','hhi','avg_size']
}

results_all = {}
for name, feats in feature_sets.items():
    Xw = weighted_X(kmeans_input, feats, weights)  # weights는 미리 계산해둔 std/합
    print(f"=== {name} ===")
    results_all[name] = find_best_k(Xw, max_k=8, name=name)

"""모든 경우의 클러스터의 elbow method와 실루엣점수가 동일하게 나오고 있지만, 이전에 했던 것에 비해서 실루엣점수가 월등히 좋아짐"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

# ----------------------------------
# 설정
# ----------------------------------
feature_sets = {
    "ocf_volume": ['ocf','volume'],
    "ocf_volume_hhi": ['ocf','volume','hhi'],
    "ocf_volume_avgSize": ['ocf','volume','avg_size'],
    "ocf_volume_hhi_avgSize": ['ocf','volume','hhi','avg_size'],
}
BEST_K = 4

# 안전장치: pq 없으면 생성
df = kmeans_input.copy()
if 'pq' not in df.columns:
    df['pq'] = df['ocf'] * df['volume']

if 'podCode' not in df.columns:
    # 포트 코드가 없으면 인덱스를 이름처럼 사용
    df['podCode'] = df.index.astype(str)

def weighted_X(df, feats):
    # 회사안: 원자료 기준 표준편차/합 → 가중치
    stds = df[feats].std(ddof=0)
    w = (stds / stds.sum()).values
    # 스케일링 후 가중치 적용
    X_scaled = MinMaxScaler().fit_transform(df[feats])
    return X_scaled * w, w

def run_kmeans_plot(name, feats):
    Xw, w = weighted_X(df, feats)

    km = KMeans(n_clusters=BEST_K, n_init=20, random_state=42)
    labels = km.fit_predict(Xw)
    df[f'cluster_{name}'] = labels

    # ---------- (1) P-Q 산점도 ----------
    plt.figure(figsize=(7,6))
    for c in range(BEST_K):
        m = (labels == c)
        plt.scatter(df.loc[m, 'ocf'], df.loc[m, 'volume'], s=18, alpha=0.7, label=f'C{c}')
    plt.xlabel('OCF (price)'); plt.ylabel('Volume')
    plt.title(f'P-Q scatter by cluster | {name} (k={BEST_K})')
    plt.grid(True); plt.legend()
    plt.show()

    # ---------- (2) 클러스터별 Top5 항만 (pq_sum) ----------
    # cluster × podCode 별 pq 합계
    g = (df.groupby([f'cluster_{name}','podCode'], as_index=False)['pq']
            .sum().rename(columns={'pq':'pq_sum'}))
    top5_list = []
    for c in range(BEST_K):
        top5 = (g[g[f'cluster_{name}']==c]
                .nlargest(5, 'pq_sum')
                .assign(cluster=c))
        top5_list.append(top5)
    top5_all = pd.concat(top5_list, ignore_index=True)

      # 막대그래프 (2x2 서브플롯)
    fig, axes = plt.subplots(2, 2, figsize=(12,8))
    axes = axes.ravel()
    for c in range(BEST_K):
        sub = top5_all[top5_all['cluster']==c].sort_values('pq_sum')
        ax = axes[c]
        ax.barh(sub['podCode'], sub['pq_sum'])
        # 제목 수정
        ax.set_title(f'클러스터 {c} 대표 항만 Top5')
        ax.set_xlabel('pq_sum'); ax.set_ylabel('podCode')
        ax.grid(True, axis='x')
    plt.tight_layout(); plt.show()


    # 표 형태로도 확인하고 싶으면 반환
    return top5_all.sort_values(['cluster','pq_sum'], ascending=[True, False])

# ---------------------------
# 실행: 4개 피처셋 모두
# ---------------------------
top5_tables = {}
for nm, feats in feature_sets.items():
    print(f'=== {nm} (k={BEST_K}) ===')
    top5_tables[nm] = run_kmeans_plot(nm, feats)

# 필요시: 어떤 조합들끼리 라벨이 정말 다른지 교차표로 확인
# 예) ocf_volume vs ocf_volume_hhi
pd.crosstab(df['cluster_ocf_volume'], df['cluster_ocf_volume_hhi'])

# 확인할 항만코드 사전 정의
sample_ports = {
    "US_WestCoast": ["USLAX","USOAK","USSEA","USPDX"],
    "China_North": ["CNTSN","CNTAO","CNDLC"],  # 텐진, 칭다오, 다롄
    "China_South": ["CNSHA","CNNGB","CNXMN","CNSHK"],  # 상하이, 닝보, 샤먼, 심천
    "Southeast_Asia": ["SGSIN","VNHPH","VNSGN"]  # 싱가포르, 하이퐁, 호치민
}

# 분석 대상 클러스터
fs_name = "ocf_volume_hhi_avgSize"
cluster_col = f'cluster_{fs_name}'

# 미리 클러스터 컬럼 생성 (없으면)
if cluster_col not in df.columns:
    Xw, w = weighted_X(df, feature_sets[fs_name])
    km = KMeans(n_clusters=BEST_K, n_init=20, random_state=42)
    df[cluster_col] = km.fit_predict(Xw)

# 결과 저장
results = {}
for case, ports in sample_ports.items():
    sub = (
        df[df['podCode'].isin(ports)]
        .loc[:, ['podCode','ocf','volume','pq','hhi','avg_size',cluster_col]]
        .copy()
    )
    results[case] = sub

    print(f"\n=== {case} ===")
    if sub.empty:
        print("해당 항만코드 없음")
    else:
        print(sub.sort_values(cluster_col))

# (옵션) 전체 확인용 병합 테이블
all_samples = pd.concat(results.values(), keys=results.keys())
print("\n=== 전체 샘플 항만 클러스터 소속 ===")
print(all_samples[['podCode','cluster_ocf_volume_hhi_avgSize']])

# ============================================
# 중국 + 미국 주요 항만: 업계 기대 포지션 vs 실제 클러스터 결과 비교
# ============================================

# 분석 대상 클러스터
fs_name = "ocf_volume_hhi_avgSize"
cluster_col = f'cluster_{fs_name}'

# 없으면 생성
if cluster_col not in df.columns:
    Xw, w = weighted_X(df, feature_sets[fs_name])
    km = KMeans(n_clusters=BEST_K, n_init=20, random_state=42)
    df[cluster_col] = km.fit_predict(Xw)

# --------------------------------------------
# 1) 업계에서 기대되는 포지션 정의
# --------------------------------------------
expectations = {
    # 중국
    "HKHKG": "고집중·저물량 (쇠퇴형 환적항, 별도 클러스터 기대)",
    "CNSHA": "저집중·고물량 (초대형 허브, Ningbo와 묶일 기대)",
    "CNNGB": "중간집중·고물량 (보완 허브, Shanghai와 묶일 기대)",

    # 미국
    "USLAX": "저집중·고물량 (주요 허브, LGB와 함께 묶일 기대)",
    "USLGB": "저집중·고물량 (주요 허브, LAX와 함께 묶일 기대)",
    "USOAK": "중간집중·저물량 (중견항, SEA와 비슷 그룹 기대)",
    "USSEA": "고집중·중저물량 (PNW 게이트웨이, OAK와 묶일 가능)",
    "USTIW": "고집중·중저물량 (PNW 게이트웨이, OAK와 묶일 가능)",
    "USPDX": "초고집중·저물량 (niche 항만, 별도 클러스터 기대)",
}

# --------------------------------------------
# 2) 실제 데이터에서 클러스터 확인
# --------------------------------------------
ports_to_check = list(expectations.keys())

sub = (
    df[df['podCode'].isin(ports_to_check)]
    .loc[:, ['podCode','ocf','volume','pq','hhi','avg_size',cluster_col]]
    .copy()
)

# --------------------------------------------
# 3) 결과 테이블 정리
# --------------------------------------------
result_table = sub.copy()
result_table['expected_position'] = result_table['podCode'].map(expectations)

# 보기 좋게 정렬
result_table = result_table.sort_values(cluster_col).reset_index(drop=True)

# --------------------------------------------
# 4) 출력
# --------------------------------------------
print("\n=== 중국/미국 주요 항만: 기대 포지션 vs 실제 클러스터 ===")
print(result_table[['podCode','expected_position',cluster_col,'ocf','volume','hhi','avg_size']])

# ---------------------------------------------------------
# 클러스터 이름 매핑
# ---------------------------------------------------------
cluster_name_map = {
    0: "분산형 안정 그룹",
    1: "저물량·고운임 그룹",
    2: "초저운임·초고물량 그룹",
    3: "대형계약 중심의 그룹",
}

def make_top10_table(name, feats, k=BEST_K):
    """
    1) (가중치+MinMax)로 스케일링 → KMeans 라벨 생성
    2) 클러스터×podCode별 pq 합계로 Top10 선정
    3) 2열 표(DataFrame) 반환: [클러스터명, Top10 항만 나열]
    """
    Xw, w = weighted_X(df, feats)
    km = KMeans(n_clusters=k, n_init=20, random_state=42)
    labels = km.fit_predict(Xw)
    df[f'cluster_{name}'] = labels

    # pq_sum 집계
    g = (df.groupby([f'cluster_{name}', 'podCode'], as_index=False)['pq']
            .sum().rename(columns={'pq':'pq_sum'}))

    # 클러스터별 Top10 뽑고, podCode를 문자열로 열거
    rows = []
    for c in range(k):
        sub = (g[g[f'cluster_{name}'] == c]
               .nlargest(10, 'pq_sum')
               .sort_values('pq_sum', ascending=False))
        ports = ", ".join(sub['podCode'].tolist())
        rows.append({
            "클러스터": f"{cluster_name_map.get(c, f'Cluster {c}')}",
            "Top10 항만": ports
        })

    top10_table = pd.DataFrame(rows)
    return top10_table

# ----------------------------------
# 실행 예시: 원하는 피처셋 1개만 만들고 싶을 때
# ----------------------------------
top10_table_ocf_vol_hhi_avg = make_top10_table("ocf_volume_hhi_avgSize",
                                               feature_sets["ocf_volume_hhi_avgSize"])
print(top10_table_ocf_vol_hhi_avg)

# ----------------------------------
# (옵션) 모든 피처셋에 대해 생성
# ----------------------------------
all_top10_tables = {}
for nm, feats in feature_sets.items():
    all_top10_tables[nm] = make_top10_table(nm, feats)
    print(f"\n=== {nm} ===")
    display(all_top10_tables[nm])  # 주피터/노트북이면 보기 좋게 표시

"""# 로그버전(volume)

지표별 표준편차 확인
"""

import matplotlib.pyplot as plt

# 사용할 피처
features = ['ocf','volume','hhi','avg_size']

# 지표별 평균, 표준편차 계산
stats = kmeans_input[features].agg(['mean','std']).T
stats['var'] = stats['std']**2

#지표별 히스토그램
plt.figure(figsize=(12,8))

for i, col in enumerate(features, 1):
    plt.subplot(2,2,i)
    plt.hist(kmeans_input[col], bins=30, color='steelblue', alpha=0.7)
    plt.axvline(kmeans_input[col].mean(), color='red', linestyle='dashed', linewidth=1)
    plt.title(f"{col} (mean={kmeans_input[col].mean():.2f}, std={kmeans_input[col].std():.2f})")

plt.tight_layout()
plt.show()

print(stats)

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# ----------------------------
# 1) 원본 데이터 복사 + 로그 변환
# ----------------------------
df_log = kmeans_input.copy()
df_log['volume_log'] = np.log1p(df_log['volume'])

# ----------------------------
# 2) feature set 정의 (log 적용)
# ----------------------------
feature_sets_log = {
    "ocf_volume": ['ocf','volume_log'],
    "ocf_volume_hhi": ['ocf','volume_log','hhi'],
    "ocf_volume_avgSize": ['ocf','volume_log','avg_size'],
    "ocf_volume_hhi_avgSize": ['ocf','volume_log','hhi','avg_size']
}

# ----------------------------
# 3) weighted_X 함수
# ----------------------------
def weighted_X(df, feats):
    # 피처셋별 std로 가중치 다시 계산
    stds = df[feats].std(ddof=0)
    w = (stds / stds.sum()).values
    X_scaled = MinMaxScaler().fit_transform(df[feats])
    return X_scaled * w, w

# ----------------------------
# 4) k 탐색 함수
# ----------------------------
def find_best_k(X, max_k=8, name=""):
    Ks, inertias, sils = [], [], []
    for k in range(2, max_k+1):
        km = KMeans(n_clusters=k, n_init=20, random_state=42)
        labels = km.fit_predict(X)
        Ks.append(k)
        inertias.append(km.inertia_)
        sils.append(silhouette_score(X, labels))

    fig, axes = plt.subplots(1,2, figsize=(12,5))
    axes[0].plot(Ks, inertias, marker='o')
    axes[0].set_title(f"{name} | Elbow (Inertia)")
    axes[0].set_xlabel("k"); axes[0].set_ylabel("Inertia"); axes[0].grid(True)
    axes[1].plot(Ks, sils, marker='o', color='orange')
    axes[1].set_title(f"{name} | Silhouette")
    axes[1].set_xlabel("k"); axes[1].set_ylabel("Silhouette score"); axes[1].grid(True)
    plt.tight_layout(); plt.show()

    return pd.DataFrame({"k": Ks, "inertia": inertias, "silhouette": sils})

# ----------------------------
# 5) 실행
# ----------------------------
results_all_log = {}
for name, feats in feature_sets_log.items():
    Xw, w = weighted_X(df_log, feats)
    print(f"=== {name} (log version) ===")
    results_all_log[name] = find_best_k(Xw, max_k=8, name=name)

"""완전 별룬데...

## 클러스터 4개의 경우  비교
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

# ----------------------------------
# 설정
# ----------------------------------
feature_sets = {
    "ocf_volume": ['ocf','volume'],
    "ocf_volume_hhi": ['ocf','volume','hhi'],
    "ocf_volume_avgSize": ['ocf','volume','avg_size'],
    "ocf_volume_hhi_avgSize": ['ocf','volume','hhi','avg_size'],
}
BEST_K = 3

# 안전장치: pq 없으면 생성
df = kmeans_input.copy()
if 'pq' not in df.columns:
    df['pq'] = df['ocf'] * df['volume']

if 'podCode' not in df.columns:
    # 포트 코드가 없으면 인덱스를 이름처럼 사용
    df['podCode'] = df.index.astype(str)

def weighted_X(df, feats):
    # 회사안: 원자료 기준 표준편차/합 → 가중치
    stds = df[feats].std(ddof=0)
    w = (stds / stds.sum()).values
    # 스케일링 후 가중치 적용
    X_scaled = MinMaxScaler().fit_transform(df[feats])
    return X_scaled * w, w

def run_kmeans_plot(name, feats):
    Xw, w = weighted_X(df, feats)

    km = KMeans(n_clusters=BEST_K, n_init=20, random_state=42)
    labels = km.fit_predict(Xw)
    df[f'cluster_{name}'] = labels

    # ---------- (1) P-Q 산점도 ----------
    plt.figure(figsize=(7,6))
    for c in range(BEST_K):
        m = (labels == c)
        plt.scatter(df.loc[m, 'ocf'], df.loc[m, 'volume'], s=18, alpha=0.7, label=f'C{c}')
    plt.xlabel('OCF (price)'); plt.ylabel('Volume')
    plt.title(f'P-Q scatter by cluster | {name} (k={BEST_K})')
    plt.grid(True); plt.legend()
    plt.show()

    # ---------- (2) 클러스터별 Top5 항만 (pq_sum) ----------
    # cluster × podCode 별 pq 합계
    g = (df.groupby([f'cluster_{name}','podCode'], as_index=False)['pq']
            .sum().rename(columns={'pq':'pq_sum'}))
    top5_list = []
    for c in range(BEST_K):
        top5 = (g[g[f'cluster_{name}']==c]
                .nlargest(5, 'pq_sum')
                .assign(cluster=c))
        top5_list.append(top5)
    top5_all = pd.concat(top5_list, ignore_index=True)

    # 막대그래프 (2x2 서브플롯)
       # 막대그래프 (2x2 서브플롯)
    fig, axes = plt.subplots(2, 2, figsize=(12,8))
    axes = axes.ravel()
    for c in range(BEST_K):
        sub = top5_all[top5_all['cluster']==c].sort_values('pq_sum')
        ax = axes[c]
        ax.barh(sub['podCode'], sub['pq_sum'])
        # 제목 수정
        ax.set_title(f'클러스터 {c} 대표 항만 Top5')
        ax.set_xlabel('pq_sum'); ax.set_ylabel('podCode')
        ax.grid(True, axis='x')
    plt.tight_layout(); plt.show()


    # 표 형태로도 확인하고 싶으면 반환
    return top5_all.sort_values(['cluster','pq_sum'], ascending=[True, False])

# ---------------------------
# 실행: 4개 피처셋 모두
# ---------------------------
top5_tables = {}
for nm, feats in feature_sets.items():
    print(f'=== {nm} (k={BEST_K}) ===')
    top5_tables[nm] = run_kmeans_plot(nm, feats)

"""## 데이터 추출"""

# 노선별 기본 지표 (고정값)
route_profile = (
    kmeans_input
      .groupby('podCode', as_index=False)
      .agg(
          volume=('volume','mean'),
          ocf=('ocf','mean'),
          hhi=('hhi','mean'),
          avg_size=('avg_size','mean')
      )
)

# 클러스터 라벨만 추출
cluster_labels = kmeans_input[['podCode',
                               'ocf_volume_cluster',
                               'ocf_volume_hhi_cluster',
                               'ocf_volume_avgSize_cluster',
                               'ocf_volume_hhi_avgSize_cluster']].drop_duplicates()

# 최종 병합
export_df = route_profile.merge(cluster_labels, on='podCode', how='left')
export_df.head()

export_df.to_excel("cluster_results.xlsx", index=False)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# --- 0) podCode가 인덱스에만 있으면 컬럼으로 꺼내기
if 'podCode' not in kmeans_input.columns and getattr(kmeans_input.index, 'name', None) == 'podCode':
    kmeans_input = kmeans_input.reset_index()

# --- 1) 노선(podCode) 단위로 정리하는 유틸
def to_route_level(df, cols):
    """podCode 단위가 아니면 집계해서 노선별 하나의 행으로 만든다."""
    use = ['podCode'] + cols
    sub = df[use].dropna().copy()
    # podCode가 중복이면 집계, 아니면 중복 제거만
    if sub.duplicated('podCode').any():
        agg_map = {c: ('mean' if c in ['ocf', 'hhi', 'avg_size'] else 'sum') for c in cols}
        # volume은 합계, 나머지는 평균(우리 표준 규칙)
        agg_map.update({'volume': 'sum'} if 'volume' in cols else {})
        sub = sub.groupby('podCode', as_index=False).agg(agg_map)
    else:
        sub = sub.drop_duplicates(subset=['podCode'])
    return sub

# --- 2) 엑셀로 내보내기 (피처셋별 시트: raw + scaled)
out_path = "kmeans_features_by_route_with_scaled.xlsx"
with pd.ExcelWriter(out_path, engine="openpyxl") as w:
    for name, fs in feature_sets.items():
        # (a) 노선별 원본 표
        route_df = to_route_level(kmeans_input, fs)

        # (b) 스케일링 (피처셋별로 fit)
        scaler = MinMaxScaler()
        scaled_vals = scaler.fit_transform(route_df[fs])
        scaled_cols = [f"{c}_scaled" for c in fs]
        scaled_df = pd.DataFrame(scaled_vals, columns=scaled_cols, index=route_df.index)

        # (c) 합치기 → podCode + raw + scaled
        merged = pd.concat([route_df[['podCode'] + fs], scaled_df], axis=1)

        # (d) 시트 저장
        merged.to_excel(w, sheet_name=name, index=False)

        # (e) 참고용: 스케일러 min/max 기록 시트(선택)
        meta = pd.DataFrame({"feature": fs,
                             "data_min_": scaler.data_min_,
                             "data_max_": scaler.data_max_})
        meta.to_excel(w, sheet_name=f"{name}__scaler_info", index=False)

print(f"저장 완료: {out_path}")



