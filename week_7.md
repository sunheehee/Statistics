# 통계학 7주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_7th_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

7주차는 `3부. 데이터 분석하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.


## Statistics_7th_TIL

### 3부. 데이터 분석하기
### 13.머신러닝 분석 방법론
### 14.모델 평가



## Study Schedule

|주차 | 공부 범위     | 완료 여부 |
|----|----------------|----------|
|1주차| 1부 p.2~56     | ✅      |
|2주차| 1부 p.57~79    | ✅      | 
|3주차| 2부 p.82~120   | ✅      | 
|4주차| 2부 p.121~202  | ✅      | 
|5주차| 2부 p.203~254  | ✅      | 
|6주차| 3부 p.300~356  | ✅      | 
|7주차| 3부 p.357~615  | ✅      | 

<!-- 여기까진 그대로 둬 주세요-->

# 13.머신러닝 분석 방법론

```
✅ 학습 목표 :
* 선형 회귀와 다항 회귀를 비교하고, 데이터를 활용하여 적절한 회귀 모델을 구축할 수 있다. 
* 로지스틱 회귀 분석의 개념과 오즈(Odds)의 의미를 설명하고, 분류 문제에 적용할 수 있다.
* k-means 알고리즘의 원리를 설명하고, 적절한 군집 개수를 결정하여 데이터를 군집화할 수 있다.
```

## 13.1. 선형 회귀분석과 Elastic Net(예측모델)
### 선형회귀분석이란?
목적: 하나 이상의 독립변수(X)가 종속변수(Y)에 어떤 영향을 미치는지 분석하고, 이를 기반으로 예측 모델을 만드는 것.

### 회귀선의 의미
관측값과 예측값의 차이를 최소화하는 직선

이 차이를 최소화하는 방식은 **최소제곱법(Least Squares Estimation)** 으로 구함

✅**선형회귀: 관측값과 가장 가까운 직선을 찾아내는 과정**

### 전제조건
1. 잔차의 정규성: 예측 오차들이 정규분포를 따라야 함

    ✔️ Q-Q Plot, Shapiro-Wilk test 등으로 검정

2. 잔차의 등분산성: 모든 독립변수에서 오차의 분산이 일정해야 함

3. 독립성: 독립변수들 간 상관관계가 없어야 함

    ✔️VIF(Variance Inflation Factor)로 확인(다중공선성 문제)

4. 선형성: 독립변수와 종속변수 간 관계가 선형이어야 함

### 다항 회귀
- 단순 회귀: 독립변수가 하나
- 다중 회귀: 독립변수가 두 개 이상

### 다중 회귀의 평가 지표
- r2 score(결정계수): 종속변수의 변동 중 몇 %를 설명하는지를 나타냄
- adj r2 score(수정된 결정계수): 변수 수를 고려한 보정 지표

    ✔️독립변수가 많을수록 Adj. R²을 보는 것이 중요
- F-검정: 전체 회귀모형이 유의한지 확인
- P-value: 각 독립변수의 유의미성 판단

### 변수 선택 기법
모델 성능 향상과 해석력을 높이기 위해 필요

- 전진 선택법: 유의미한 변수를 하나씩 추가
- 후진 제거법: 전체 변수에서 의미 없는 것부터 제거
- 단계별 선택법: 추가와 제거를 병행하여 선택

### 과적합 방지와 정규화
**Ridge 회귀 (L2 정규화)**
- 모든 변수 유지하면서 회귀계수를 작게 조정
- 예측력은 좋지만 해석이 어려움

**Lasso 회귀 (L1 정규화)**
- 중요하지 않은 변수의 계수를 0으로 만들어 변수 선택 기능까지 수행
- 해석력이 뛰어남

**Elastic Net**
- Ridge + Lasso

### 다항 회귀 분석
선형 회귀로 설명되지 않는 비선형 관계를 곡선 형태로 모델링

- 독립변수와 종속변수의 관계가 곡선형 관계일 때 변수에 **각 특성의 제곱을 추가**하여 회귀선을 곡선형으로 변환하는 모델을 의미

- 차수가 커질수록 편향은 감소, 변동성이 증가
- 분산이 늘어나고 과적합 유발 가능성 있음음

---

## 13.2. 로지스틱 회귀분석 (분류모델)

### 로지스틱 회귀분석

#### 선형회귀와의 차이점점
- 선형 회귀는 종속변수가 연속형 수치
- 로지스틱 회귀는 종속변수가 **범주형**
#### 로지스틱 회귀란?
- 목적:어떤 사건이 일어날 확률(예: 합격/불합격, 구매/미구매 등)을 예측하거나, 여러 범주 중 하나를 분류


### 오즈란?
어떤 사건이 발생할 확률(p)과 발생하지 않을 확률(1-p)의 비


### 로지스틱 회귀의 분류 적용
모델이 출력하는 확률 값을 기준으로 분류
- 0.5 이상이면 1(구매), 미만이면 0(구매 X)
- 이 임계값(기준)은 상황에 따라 임의로로 조정 가능

✅ **다중 범주**일 경우(예: 클래스가 3개 이상):

**다항 로지스틱 회귀(Multinomial Logistic Regression) 사용**

각 클래스에 대해 기준 클래스를 설정하고 K-1개의 로지스틱 식을 만듦

### 로지스틱 회귀 결과 해석
계수(β)는 독립변수가 변할 때 오즈의 로그 변화량을 의미함

계수의 부호는 영향의 방향:

- 양수 → 변수 증가 시 사건 발생 가능성 ↑
- 음수 → 변수 증가 시 사건 발생 가능성 ↓

Odds Ratio (OR): 각 계수에 대해 오즈로 해석

예: OR = 1.22 → 해당 변수가 1 증가할 때 사건이 발생할 오즈가 22% 증가


----

## 13.8. k-means 클러스터링(군집모델)

### k-means알고리즘의 원리
1. 초기 중심점(k개) 임의 설정

    군집 수 K개개를 지정하면, 알고리즘이 K개의 중심점을 무작위로 선택합니다

2. 각 데이터 → 가장 가까운 중심점에 할당

    각 데이터는 유클리드 거리 등 기준에 따라 가장 가까운 중심점에 할당되어 군집이 형성됨

3. 군집별 중심점 재계산

    군집에 속한 데이터들의 평균 위치를 계산해 새로운 중심점으로 설정


4. 중심점 이동 → 수렴 여부 확인

    중심점이 더 이상 이동하지 않거나 지정된 반복 횟수에 도달하면 종료됩니다.

📌 핵심 개념: 중심점과 각 데이터 간의 거리 합(오차 제곱합, Inertia)을 최소화하는 방향으로 군집을 형성함

### 군집 수 결정 방법
1. Elbow Method (엘보우 기법)
- 군집 수에 따른 총 거리합(Inertia) 변화를 시각화
- 급격히 감소하다 완만해지는 지점(Knee point)이 적정 군집 수

2. 실루엣 계수(Silhouette Coefficient)
- 군집 내 밀집도와 군집 간 분리도 비교
- 값이 1에 가까울수록 군집화 품질이 높음


----------------------

# 14. 모델 평가

```
✅ 학습 목표 :
* 유의확률(p-value)을 해석할 때 주의할 점을 설명할 수 있다.
* 분석가가 올바른 주관적 판단을 위한 필수 요소를 식별할 수 있다.
```

## 14.3. 회귀성능 평가지표
### 1.  R-Square (결정계수)
독립변수가 종속변수의 분산을 얼마나 설명하는지 나타냄 (0~1 사이)

- 해석: 1에 가까울수록 예측력이 높음
- 주의: 변수를 많이 넣을수록 R²는 무조건 증가함 → 과적합 위험

### Adjusted R-Square (수정된 결정계수)
R²에 모델의 변수 개수(p) 보정을 가한 지표

- 해석: 과도한 변수 추가를 방지함 → 과적합 방어용
- R²과 차이점: 변수 수가 증가해도 모델 설명력이 부족하면 Adj R²는 감소함

### RMSE (Root Mean Squared Error)
예측값과 실제값의 제곱 오차 평균의 제곱근

- 해석: 단위 그대로 유지되며, 이상치에 민감
- 사용 시기: 이상치가 중요할 때, 예측값의 크기를 직접 비교할 때

### MAE (Mean Absolute Error)
예측값과 실제값의 절대 오차 평균

- 특징: RMSE보다 이상치에 덜 민감
- 해석: 직관적이고 해석 쉬움 (평균적으로 얼마나 틀렸는가)

### MAPE (Mean Absolute Percentage Error)
예측 오차를 백분율로 환산한 지표

- 특징: 단위 없이 퍼센트로 해석 가능
- 주의: 실제값이 0에 가까우면 폭발적 오류 가능

### AIC / BIC
- 공통점: 낮을수록 좋은 모델
- 차이점: BIC는 변수 수가 많을수록 더 강한 패널티


## 14.6. 유의확률의 함정
### 유의확률(p-value)의 의미
관측된 데이터가 귀무가설(null hypothesis) 하에서 얼마나 극단적인지를 나타내는 확률

⚠️ 일반적으로 p < 0.05일 경우 통계적으로 유의하다고 판단하지만, 이는 임의적인 기준

### p-value의 주요 함정
1. **표본 수가 크면 p값은 쉽게 작아짐**

    실제 효과가 작거나 무의미해도, <u>**표본 수(n)가 크면 p값은 작아짐**</u>
        
        예: 연소득 차이 10만 원 → n=30이면 p=0.13 (유의X), n=300이면 p=0.003 (유의O)
        
        ➡️ 통계적 유의성 ≠ 실질적 의미

2. **귀무가설 기각은 효과의 존재를 말할 뿐, 중요성이나 방향성은 말하지 않음**

    p값은 “차이 존재 여부”만 알려줄 뿐, <u>**그 차이의 크기나 중요성은 평가하지 않음**</u>
    
    ➡️ 효과 크기(effect size), 신뢰구간(CI), 그래프를 함께 제시해야 해석이 정확해짐

3. **p값에만 의존한 연구 결과는 재현성이 낮음**

    반복 연구 시 결과가 뒤바뀔 수 있음.


4. **p-hacking 위험**

    여러 변수, 조건을 바꿔가며 유의한 결과가 나올 때까지 분석을 반복하면 p-hacking 위험 발생.
    
    ➡️이는 데이터에 “우연히” 맞춰진 결과일 수 있음.




## 14.7. 분석가의 주관적 판단과 스토리텔링

### 분석가의 주관적 판단이 중요한 이유
1. 모델의 한계

    기계학습, AI 모델은 패턴을 예측할 수 있지만, 사람의 심리나 사회적 맥락까지 고려하기는 어렵다.

2. 데이터에 드러나지 않는 인간의 행동

    고객의 행동은 단순히 데이터로는 예측이 어려운 동기와 배경을 가진다.
    
        예: 특정 연령층에서 자동차 구매율이 높은 이유는 실제로는 할머니가 손자에게 차를 사주는 문화 때문.

### 분석가에게 필요한 3가지 역량
1. 도메인 지식

    도메인을 모르면 데이터 해석이 왜곡될 수 있다.

2. 통계 데이터 분석 지식

    데이터의 특성을 깊이 있게 이해하고, 단순히 통계 수치에만 의존하지 않는 분석이 가능해진다.

3. 커뮤니케이션/스토리텔링

    분석 결과를 의사결정자나 현업 담당자와 소통하면서 해석 및 방향 보완이 가능해진다.



<br>
<br>

# 확인 문제

## **문제 1. 선형 회귀**

> **🧚 칼 피어슨의 아버지와 아들의 키 연구 결과를 바탕으로, 다음 선형 회귀식을 해석하세요.**  
> 칼 피어슨(Karl Pearson)은 아버지(X)와 아들(Y)의 키를 조사한 결과를 바탕으로 아래와 같은 선형 회귀식을 도출하였습니다. 아래의 선형 회귀식을 보고 기울기의 의미를 설명하세요. 
>  
> **ŷ = 33.73 + 0.516X**  
>   
> - **X**: 아버지의 키 (cm)  
> - **ŷ**: 아들의 예상 키 (cm)  

```
위 회귀식에서의 기울기: 0.516(양의 선형관계)
이는 아버지의 키가 1cm 증가할 때, 아들의 예상 키도 평균적으로 0.516cm 증가한다는것을 의미한다.

```
---

## **문제 2. 로지스틱 회귀**  

> **🧚 다트비에서는 학생의 학업 성취도를 예측하기 위해 다항 로지스틱 회귀 분석을 수행하였습니다. 학업 성취도(Y)는 ‘낮음’, ‘보통’, ‘높음’ 3가지 범주로 구분되며, 독립 변수는 주당 공부 시간(Study Hours)과 출석률(Attendance Rate)입니다. 단, 기준범주는 '낮음' 입니다.**   

| 변수 | Odds Ratio Estimates | 95% Wald Confidence Limits |  
|------|----------------------|--------------------------|  
| Study Hours | **2.34** | (1.89, 2.88) |  
| Attendance Rate | **3.87** | (2.92, 5.13) |  

> 🔍 Q1. Odds Ratio Estimates(오즈비, OR)의 의미를 해석하세요.

<!--변수 Study Hours의 오즈비 값이 2.34라는 것과 Attendance Rate의 오즈비 값이 3.87이라는 것이 각각 무엇을 의미하는지 구체적으로 생각해보세요.-->

```
Study Hours의 오즈비가 2.34라는 것은, 주당 공부 시간이 1시간 증가할 때, 학생이 학업 성취도가 ‘보통’ 또는 ‘높음’일 확률이 ‘낮음’에 비해 2.34배 높아진다는 의미로 해석 가능

Attendance Rate의 오즈비가 3.87이면, 출석률이 1증가할 때, 학업 성취도가 ‘보통’ 또는 ‘높음’일 확률이 3.87배 높아진다고 해석 가능
```

> 🔍 Q2. 95% Wald Confidence Limits의 의미를 설명하세요.

```
오즈비의 신뢰구간은 해당 변수의 영향이 통계적으로 신뢰할 수 있는 범위를 말한다. 

✔️ Study Hours의 신뢰구간: 오즈비가 1.89 이상 2.88 이하일 확률이 95%이며, 이는 효과가 통계적으로 유의미함을 의미합니다.

✔️ Attendance Rate의 신뢰구간 (2.92, 5.13): 오즈비가 95% 신뢰수준에서 이 범위에 있을 것으로 예상되며, 역시 유의미한 양의 영향을 준다고 해석 가능
```

> 🔍 Q3. 이 분석을 기반으로 학업 성취도를 향상시키기 위한 전략을 제안하세요.
<!--Study Hours와 Attendance Rate 중 어느 변수가 학업 성취도에 더 큰 영향을 미치는지를 고려하여, 학업 성취도를 향상시키기 위한 효과적인 전략을 구체적으로 제시해보세요.-->

```
분석 결과에서 Attendance Rate의 오즈비가 더 높고, 신뢰구간도 더 크다. 즉, 출석률이 성취도에 더 큰 영향을 미친다고 해석할 수 있다. 이를 이용해서 학업 성취도를 향상시킬 전략을 하나 제안해본다.

<학생 참여 수업 프로그램 진행하기>

학생들이 원하는 수업방식을 제안받아 추첨식으로 랜덤하게 진행해보는 것!

학생들의 창의성 증진과, 학생간의 유대감, 친화력 증진과 수업에 대한 기대감, 흥미를 불러일으킬 수 있을것으로 기대된다. 


```

---


## **문제 3. k-means 클러스터링**

> **🧚 선교는 고객을 유사한 그룹으로 분류하기 위해 k-means 클러스터링을 적용했습니다. 초기에는 3개의 군집으로 설정했지만, 결과가 만족스럽지 않았습니다. 선교가 최적의 군집 수를 찾기 위해 사용할 수 있는 방법을 한 가지 이상 제시하고 설명하세요.**

```
최적의 군집 수를 찾는 방법에는 2가지가 있다.

✅ 엘보우 기법
- 각 K값에 대해 클러스터링을 수행하고, 군집 내 제곱거리 합(Inertia)을 계산

- 이 값을 그래프로 나타냈을 때, 급격히 감소하다가 완만해지는 지점의 K값을 최적 군집 수로 선택



✅ 실루엣 개수

- 각 데이터 포인트가 본인이 속한 군집과 얼마나 잘 어울리는지 측정하는 지표.

- 실루엣 계수가 1에 가까울수록 군집이 잘 형성되었다는 뜻이며, 여러 K값에 대해 계산하여 가장 높은 값을 주는 K를 선택

선교 화이팅!
```

### 🎉 수고하셨습니다.